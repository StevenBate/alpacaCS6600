{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13fdc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import shap\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20db1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Helpers & output folders\n",
    "# -------------------------\n",
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUT_DIR, \"figs\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUT_DIR, \"models\"), exist_ok=True)\n",
    "\n",
    "def save_fig(name):\n",
    "    path = os.path.join(OUT_DIR, \"figs\", name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "    print(f\"Saved figure: {path}\")\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "615b19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Data loading: flexible\n",
    "# -------------------------\n",
    "def load_merged_df():\n",
    "    csv_path = \"data/raw/data.csv\"\n",
    "    df = pd.read_csv(csv_path, parse_dates=['timestamp', 'date'], infer_datetime_format=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99dd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_engineer(df, debug=False):\n",
    "    \"\"\"\n",
    "    Input: merged_df (as produced by your EDA)\n",
    "    Steps:\n",
    "      - Ensure timestamp/date columns\n",
    "      - Create target = next-day high\n",
    "      - Lag features, rolling stats\n",
    "      - Handle missing values\n",
    "      - Standardize features (scaler fitted on training set later)\n",
    "    Returns: processed dataframe (no scaling applied), feature list\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # ensure datetime\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    if 'date' in df.columns and not np.issubdtype(df['date'].dtype, np.datetime64):\n",
    "        try:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "        except:\n",
    "            df['date'] = pd.to_datetime(df['timestamp'].dt.date)\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['timestamp'].dt.date)\n",
    "\n",
    "    # Sort by time\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Target: tomorrow's high\n",
    "    df['target_high'] = df['high'].shift(-1)\n",
    "\n",
    "    # lag features\n",
    "    lags = [1,2,3]\n",
    "    for lag in lags:\n",
    "        df[f'close_lag_{lag}'] = df['close'].shift(lag)\n",
    "        df[f'return_lag_{lag}'] = df['daily_return'].shift(lag)\n",
    "        df[f'vol_lag_{lag}'] = df['volume'].shift(lag)\n",
    "\n",
    "    # Rolling stats\n",
    "    df['rolling_mean_5'] = df['close'].rolling(5).mean()\n",
    "    df['rolling_std_5'] = df['close'].rolling(5).std()\n",
    "    df['rolling_mean_10'] = df['close'].rolling(10).mean()\n",
    "    df['rolling_std_10'] = df['close'].rolling(10).std()\n",
    "\n",
    "    # Sentiment lags\n",
    "    df['sentiment_lag_1'] = df['avg_sentiment'].shift(1)\n",
    "\n",
    "    # Time features\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "    # Drop rows with NA in target\n",
    "    df = df.dropna(subset=['target_high']).reset_index(drop=True)\n",
    "\n",
    "    # Fill remaining missing feature values with median (simple imputer later)\n",
    "    # Keep columns list\n",
    "    feature_cols = [\n",
    "        'open','high','low','close','volume','vwap','trade_count','daily_return',\n",
    "        'rolling_mean_5','rolling_std_5','rolling_mean_10','rolling_std_10',\n",
    "        'close_lag_1','close_lag_2','close_lag_3',\n",
    "        'return_lag_1','return_lag_2','return_lag_3',\n",
    "        'vol_lag_1','vol_lag_2','vol_lag_3',\n",
    "        'sentiment_lag_1','dayofweek','month'\n",
    "    ]\n",
    "    # Some columns on user data may not exist (e.g., trade_count)\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "    if debug:\n",
    "        print(\"Feature columns:\", feature_cols)\n",
    "\n",
    "    # We'll not scale here; scaling is part of a sklearn pipeline fitted on train.\n",
    "    return df, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c195c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_only_eda(X_train_df, y_train, out_prefix=\"train\"):\n",
    "    \"\"\"\n",
    "    Perform exploration ONLY on the training set per assignment.\n",
    "    Save descriptive stats, correlation matrix, and plots.\n",
    "    \"\"\"\n",
    "    print(\"=== Training-only EDA ===\")\n",
    "    stats = X_train_df.describe().T\n",
    "    stats.to_csv(os.path.join(OUT_DIR, f\"{out_prefix}_descriptive_stats.csv\"))\n",
    "    print(f\"Saved descriptive stats to {OUT_DIR}/{out_prefix}_descriptive_stats.csv\")\n",
    "\n",
    "    # Correlation (features + target)\n",
    "    corr = X_train_df.join(y_train).corr()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(corr, annot=False, cmap='coolwarm')\n",
    "    plt.title('Training-set Correlation Matrix (features + target)')\n",
    "    save_fig(f\"{out_prefix}_corr_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Distribution plots for a handful of numeric features\n",
    "    cols = X_train_df.select_dtypes(include=[np.number]).columns.tolist()[:8]\n",
    "    for c in cols:\n",
    "        plt.figure(figsize=(5,3))\n",
    "        sns.histplot(X_train_df[c].dropna(), kde=True, bins=30)\n",
    "        plt.title(f'{c} distribution (train)')\n",
    "        save_fig(f\"{out_prefix}_dist_{c}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Scatter: sentiment_lag_1 vs target (if present)\n",
    "    if 'sentiment_lag_1' in X_train_df.columns:\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.scatterplot(x=X_train_df['sentiment_lag_1'], y=y_train)\n",
    "        plt.title('Sentiment (lag1) vs Target High (train)')\n",
    "        save_fig(f\"{out_prefix}_sentiment_vs_target.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4f0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(y_true, y_pred):\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': rmse(y_true, y_pred),\n",
    "        'R2': r2_score(y_true, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e30a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train baseline, LinearRegression, RandomForest, LightGBM, (optional) Keras.\n",
    "    Return fitted models and metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    models = {}\n",
    "\n",
    "    baseline_pred = X_test['high'].values\n",
    "    results['baseline'] = evaluate_regression(y_test, baseline_pred)\n",
    "    print(\"Baseline:\", results['baseline'])\n",
    "\n",
    "    # Pipeline to impute + scale\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    X_test_scaled = scaler.transform(imputer.transform(X_test))\n",
    "\n",
    "    # Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    lr_pred = lr.predict(X_test_scaled)\n",
    "    results['linear_regression'] = evaluate_regression(y_test, lr_pred)\n",
    "    models['linear_regression'] = ('lr', lr, imputer, scaler)\n",
    "    print(\"Linear Regression:\", results['linear_regression'])\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestRegressor(n_estimators=300, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    rf_pred = rf.predict(X_test_scaled)\n",
    "    results['random_forest'] = evaluate_regression(y_test, rf_pred)\n",
    "    models['random_forest'] = ('rf', rf, imputer, scaler)\n",
    "    print(\"Random Forest:\", results['random_forest'])\n",
    "\n",
    "    # LightGBM\n",
    "    lgbm = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.05, random_state=42)\n",
    "    lgbm.fit(X_train_scaled, y_train)\n",
    "    lgbm_pred = lgbm.predict(X_test_scaled)\n",
    "    results['lightgbm'] = evaluate_regression(y_test, lgbm_pred)\n",
    "    models['lightgbm'] = ('lgbm', lgbm, imputer, scaler)\n",
    "    print(\"LightGBM:\", results['lightgbm'])\n",
    "\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    nn = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # regression output\n",
    "    ])\n",
    "    nn.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=[])\n",
    "\n",
    "    nn.fit(X_train_scaled, y_train, validation_split=0.1, epochs=50, batch_size=32, verbose=0)\n",
    "    nn_pred = nn.predict(X_test_scaled).flatten()\n",
    "    results['keras_nn'] = evaluate_regression(y_test, nn_pred)\n",
    "    models['keras_nn'] = ('keras_nn', nn, imputer, scaler)\n",
    "    print(\"Keras NN results:\", results['keras_nn'])\n",
    "\n",
    "    # Save models\n",
    "    for name, tup in models.items():\n",
    "        tag, m, imputer_obj, scaler_obj = tup\n",
    "        joblib.dump(tup, os.path.join(OUT_DIR, \"models\", f\"{name}.joblib\"))\n",
    "    # also save results\n",
    "    res_df = pd.DataFrame(results).T\n",
    "    res_df.to_csv(os.path.join(OUT_DIR, \"model_results.csv\"))\n",
    "    print(\"Saved model results to outputs/model_results.csv\")\n",
    "    return models, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53eda397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_analysis(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Run KMeans, PCA, Agglomerative, and IsolationForest\n",
    "    \"\"\"\n",
    "    print(\"=== Unsupervised analysis ===\")\n",
    "    X_unsup = df[feature_cols].select_dtypes(include=[np.number]).fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X_unsup)\n",
    "\n",
    "    # PCA (2 components for visualization)\n",
    "    pca = PCA(n_components=2, random_state=0)\n",
    "    pcs = pca.fit_transform(Xs)\n",
    "    pca_df = pd.DataFrame(pcs, columns=['PC1','PC2'])\n",
    "    pca_df.to_csv(os.path.join(OUT_DIR, \"pca_components.csv\"))\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'])\n",
    "    plt.title('PCA (2 components)')\n",
    "    save_fig(\"unsup_pca.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # KMeans\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "    labels = kmeans.fit_predict(Xs)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=labels, palette='tab10')\n",
    "    plt.title('KMeans clusters (k=3) on PCA')\n",
    "    save_fig(\"unsup_kmeans_pca.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Agglomerative\n",
    "    agg = AgglomerativeClustering(n_clusters=3)\n",
    "    agg_labels = agg.fit_predict(Xs)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=agg_labels, palette='deep')\n",
    "    plt.title('Agglomerative clusters on PCA')\n",
    "    save_fig(\"unsup_agg_pca.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # IsolationForest for anomalies\n",
    "    iso = IsolationForest(random_state=0)\n",
    "    iso_pred = iso.fit_predict(Xs)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=iso_pred, palette='Set1')\n",
    "    plt.title('IsolationForest (anomaly detection)')\n",
    "    save_fig(\"unsup_iso_pca.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save clustering models\n",
    "    joblib.dump((kmeans, scaler), os.path.join(OUT_DIR, \"models\", \"kmeans.joblib\"))\n",
    "    joblib.dump((agg, scaler), os.path.join(OUT_DIR, \"models\", \"agg.joblib\"))\n",
    "    joblib.dump((iso, scaler), os.path.join(OUT_DIR, \"models\", \"iso.joblib\"))\n",
    "\n",
    "    return {\n",
    "        'pca': pca,\n",
    "        'kmeans': kmeans,\n",
    "        'agg': agg,\n",
    "        'isolation_forest': iso\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55639355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_validation(model, X, y, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    scores = []\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_tr, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        # simple pipeline: impute -> scale -> fit model\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_scaled = scaler.fit_transform(imputer.fit_transform(X_tr))\n",
    "        X_val_scaled = scaler.transform(imputer.transform(X_val))\n",
    "        model.fit(X_tr_scaled, y_tr)\n",
    "        pred = model.predict(X_val_scaled)\n",
    "        scores.append(rmse(y_val, pred))\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b050d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_tune_rf(X_train, y_train, n_trials=30):\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "        max_features = trial.suggest_categorical('max_features', ['sqrt','log2', 0.5, None])\n",
    "\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        # cross-validate using simple 3-fold (not time series CV for speed)\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "        scores = -cross_val_score(model, Xs, y_train, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    print(\"RF best params:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e883fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keras_nn(X_train, y_train, X_test, y_test, \n",
    "                   hidden_layers=[64, 32], dropout=0.2, \n",
    "                   lr=0.001, batch_size=32, epochs=200, patience=20):\n",
    "    \"\"\"\n",
    "    Train a simple feedforward neural network for regression.\n",
    "    \n",
    "    Returns:\n",
    "        model: trained Keras model\n",
    "        results: dict of metrics on test set\n",
    "    \"\"\"\n",
    "    # Impute and scale\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "    Xte = scaler.transform(imputer.transform(X_test))\n",
    "    \n",
    "    # Build model\n",
    "    model = Sequential()\n",
    "    input_dim = Xtr.shape[1]\n",
    "    for i, units in enumerate(hidden_layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(units, activation='relu', input_dim=input_dim))\n",
    "        else:\n",
    "            model.add(Dense(units, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='linear'))  # regression output\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        Xtr, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[es],\n",
    "        verbose=0  # change to 1 to see progress\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(Xte).flatten()\n",
    "    results = evaluate_regression(y_test, y_pred)\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(('keras_nn', model, imputer, scaler), os.path.join(OUT_DIR, \"models\", \"keras_nn.joblib\"))\n",
    "    print(\"Neural Network results:\", results)\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dcb80db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_tune_lgb(X_train, y_train, n_trials=30):\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "            'bagging_freq': 1\n",
    "        }\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "        split_idx = int(0.8 * len(Xs))\n",
    "        X_tr, X_val = Xs[:split_idx], Xs[split_idx:]\n",
    "        y_tr, y_val = y_train.iloc[:split_idx], y_train.iloc[split_idx:]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
    "        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "\n",
    "        # Train with early stopping\n",
    "        cvres = lgb.train(\n",
    "            params=param,\n",
    "            train_set=dtrain,\n",
    "            valid_sets=[dval],\n",
    "            num_boost_round=1000,\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=20),   # early stopping\n",
    "                lgb.log_evaluation(period=0)              # suppress printing\n",
    "            ]\n",
    "        )\n",
    "        return cvres.best_score['valid_0']['rmse']\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    print(\"LightGBM best params:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a63da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_explain(model_tuple, X_train, X_test, feature_names, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    model_tuple: (tag, model, imputer, scaler) saved earlier\n",
    "    \"\"\"\n",
    "    tag, model, imputer, scaler = model_tuple\n",
    "    # preprocess\n",
    "    X_train_pre = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "    X_test_pre = scaler.transform(imputer.transform(X_test))\n",
    "\n",
    "    # SHAP for tree models (RandomForest, LightGBM)\n",
    "    model_type = str(type(model)).lower()\n",
    "    explainer = None\n",
    "    if hasattr(model, 'predict'):\n",
    "        try:\n",
    "            if 'lgbm' in model_type or 'lightgbm' in model_type:\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "            elif 'randomforest' in model_type or 'forest' in model_type:\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "            elif 'sequential' in model_type or 'keras' in model_type:\n",
    "                background = X_train_pre[np.random.choice(X_train_pre.shape[0], min(100, X_train_pre.shape[0]), replace=False)]\n",
    "                explainer = shap.KernelExplainer(model.predict, background)\n",
    "                shap_values = explainer.shap_values(X_test_pre[:50], nsamples=100)\n",
    "            else:\n",
    "                explainer = shap.KernelExplainer(model.predict, X_train_pre[:100])\n",
    "        except Exception as e:\n",
    "            print(\"SHAP explainer construction failed:\", e)\n",
    "            return\n",
    "    else:\n",
    "        print(\"Model object not suitable for SHAP\")\n",
    "        return\n",
    "\n",
    "    shap_values = explainer.shap_values(X_test_pre[:200])  # limit for speed\n",
    "    # summary plot\n",
    "    plt.figure(figsize=(6,4))\n",
    "    shap.summary_plot(shap_values, X_test_pre[:200], feature_names=feature_names, show=False)\n",
    "    save_fig(f\"shap_summary_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # feature importance as table\n",
    "    mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "    fi = pd.Series(mean_abs, index=feature_names).sort_values(ascending=False)\n",
    "    fi.to_csv(os.path.join(OUT_DIR, f\"shap_feature_importance_{model_name}.csv\"))\n",
    "    print(f\"Saved SHAP feature importance for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54e3af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline():\n",
    "    df = load_merged_df()\n",
    "    df, feature_cols = preprocess_and_engineer(df, debug=True)\n",
    "\n",
    "    # train/test split: use temporal split (no shuffle)\n",
    "    n = len(df)\n",
    "    test_size = int(0.2 * n)\n",
    "    train_df = df.iloc[:-test_size].reset_index(drop=True)\n",
    "    test_df = df.iloc[-test_size:].reset_index(drop=True)\n",
    "\n",
    "    # Training-only EDA\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df['target_high']\n",
    "    training_only_eda(X_train, y_train, out_prefix=\"train\")\n",
    "\n",
    "    # Unsupervised analysis\n",
    "    unsup_models = unsupervised_analysis(train_df, feature_cols)\n",
    "\n",
    "    # Prepare test set\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df['target_high']\n",
    "\n",
    "    # Train supervised models\n",
    "    models, results = train_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # TimeSeries CV for naive RF\n",
    "    try:\n",
    "        mean_rmse, std_rmse = time_series_validation(\n",
    "            RandomForestRegressor(n_estimators=100),\n",
    "            train_df[feature_cols],\n",
    "            train_df['target_high'],\n",
    "            n_splits=5\n",
    "        )\n",
    "        print(f\"TimeSeriesCV (naive RF) RMSE: mean {mean_rmse:.4f}, std {std_rmse:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"TimeSeries CV failed:\", e)\n",
    "\n",
    "    # Hyperparameter tuning with Optuna\n",
    "    print(\"Starting Optuna tuning (this may take a while)...\")\n",
    "    try:\n",
    "        best_rf_params = optuna_tune_rf(X_train, y_train, n_trials=20)\n",
    "        best_lgb_params = optuna_tune_lgb(X_train, y_train, n_trials=20)\n",
    "    except Exception as e:\n",
    "        print(\"Optuna tuning failed or interrupted:\", e)\n",
    "        best_rf_params, best_lgb_params = None, None\n",
    "\n",
    "    # Fit tuned models\n",
    "    tuned_results = {}\n",
    "    if best_rf_params:\n",
    "        rf_tuned = RandomForestRegressor(**best_rf_params, random_state=42, n_jobs=-1)\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "        rf_tuned.fit(Xtr, y_train)\n",
    "        Xte = scaler.transform(imputer.transform(X_test))\n",
    "        rf_pred = rf_tuned.predict(Xte)\n",
    "        tuned_results['rf_tuned'] = evaluate_regression(y_test, rf_pred)\n",
    "        joblib.dump(('rf_tuned', rf_tuned, imputer, scaler), os.path.join(OUT_DIR, \"models\", \"rf_tuned.joblib\"))\n",
    "        print(\"RF tuned results:\", tuned_results['rf_tuned'])\n",
    "    if best_lgb_params:\n",
    "        lgb_tuned = lgb.LGBMRegressor(**best_lgb_params, random_state=42)\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "        lgb_tuned.fit(Xtr, y_train)\n",
    "        Xte = scaler.transform(imputer.transform(X_test))\n",
    "        lgb_pred = lgb_tuned.predict(Xte)\n",
    "        tuned_results['lgb_tuned'] = evaluate_regression(y_test, lgb_pred)\n",
    "        joblib.dump(('lgb_tuned', lgb_tuned, imputer, scaler), os.path.join(OUT_DIR, \"models\", \"lgb_tuned.joblib\"))\n",
    "        print(\"LightGBM tuned results:\", tuned_results['lgb_tuned'])\n",
    "\n",
    "    # --- Neural Network ---\n",
    "    try:\n",
    "        nn_model, nn_results = train_keras_nn(X_train, y_train, X_test, y_test)\n",
    "        tuned_results['keras_nn'] = nn_results\n",
    "    except Exception as e:\n",
    "        print(\"Neural network training failed:\", e)\n",
    "\n",
    "    # Final evaluation: combine all results\n",
    "    all_results = []\n",
    "    for name, metrics in results.items():\n",
    "        row = {'model': name}\n",
    "        row.update(metrics)\n",
    "        all_results.append(row)\n",
    "    for name, metrics in tuned_results.items():\n",
    "        row = {'model': name}\n",
    "        row.update(metrics)\n",
    "        all_results.append(row)\n",
    "\n",
    "    res_df = pd.DataFrame(all_results).set_index('model')\n",
    "    res_df.to_csv(os.path.join(OUT_DIR, \"final_evaluation_results.csv\"))\n",
    "    print(\"Saved final evaluation results to outputs/final_evaluation_results.csv\")\n",
    "\n",
    "    if not res_df.empty:\n",
    "        best_model_name = res_df['RMSE'].idxmin()\n",
    "        print(\"Best model by RMSE:\", best_model_name)\n",
    "        model_tuple = None\n",
    "        # Try to load the model object\n",
    "        try:\n",
    "            model_tuple = joblib.load(os.path.join(OUT_DIR, \"models\", f\"{best_model_name}.joblib\"))\n",
    "        except Exception:\n",
    "            try:\n",
    "                model_tuple = joblib.load(os.path.join(OUT_DIR, \"models\", f\"{best_model_name}_tuned.joblib\"))\n",
    "            except Exception:\n",
    "                if best_model_name in models:\n",
    "                    model_tuple = models[best_model_name]\n",
    "        if model_tuple:\n",
    "            try:\n",
    "                shap_explain(model_tuple, X_train, X_test, feature_cols, model_name=best_model_name)\n",
    "            except Exception as e:\n",
    "                print(\"SHAP explanation failed:\", e)\n",
    "        else:\n",
    "            print(\"Could not find model object for SHAP explanation:\", best_model_name)\n",
    "    print(\"Pipeline complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e05a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: ['open', 'high', 'low', 'close', 'volume', 'vwap', 'trade_count', 'daily_return', 'rolling_mean_5', 'rolling_std_5', 'rolling_mean_10', 'rolling_std_10', 'close_lag_1', 'close_lag_2', 'close_lag_3', 'return_lag_1', 'return_lag_2', 'return_lag_3', 'vol_lag_1', 'vol_lag_2', 'vol_lag_3', 'sentiment_lag_1', 'dayofweek', 'month']\n",
      "=== Training-only EDA ===\n",
      "Saved descriptive stats to outputs/train_descriptive_stats.csv\n",
      "Saved figure: outputs\\figs\\train_corr_matrix.png\n",
      "Saved figure: outputs\\figs\\train_dist_open.png\n",
      "Saved figure: outputs\\figs\\train_dist_high.png\n",
      "Saved figure: outputs\\figs\\train_dist_low.png\n",
      "Saved figure: outputs\\figs\\train_dist_close.png\n",
      "Saved figure: outputs\\figs\\train_dist_volume.png\n",
      "Saved figure: outputs\\figs\\train_dist_vwap.png\n",
      "Saved figure: outputs\\figs\\train_dist_trade_count.png\n",
      "Saved figure: outputs\\figs\\train_dist_daily_return.png\n",
      "Saved figure: outputs\\figs\\train_sentiment_vs_target.png\n",
      "=== Unsupervised analysis ===\n",
      "Saved figure: outputs\\figs\\unsup_pca.png\n",
      "Saved figure: outputs\\figs\\unsup_kmeans_pca.png\n",
      "Saved figure: outputs\\figs\\unsup_agg_pca.png\n",
      "Saved figure: outputs\\figs\\unsup_iso_pca.png\n",
      "Baseline: {'MAE': 1.4658800000000007, 'RMSE': 2.0086945134589276, 'R2': 0.9389245229252972}\n",
      "Linear Regression: {'MAE': 1.2057438899065198, 'RMSE': 1.5701200639665327, 'R2': 0.9626831645086711}\n",
      "Random Forest: {'MAE': 1.3841295791204344, 'RMSE': 1.8011969204224005, 'R2': 0.9508909567891225}\n",
      "LightGBM: {'MAE': 1.6431832595526772, 'RMSE': 2.0801933273298134, 'R2': 0.9344992188629831}\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Keras NN results: {'MAE': 54.34573010668945, 'RMSE': 69.26189101706241, 'R2': -71.61523893001015}\n",
      "Saved model results to outputs/model_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:29,138] A new study created in memory with name: no-name-bf99741b-92a8-47ad-999e-654ac3743430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesCV (naive RF) RMSE: mean 6.0770, std 4.1456\n",
      "Starting Optuna tuning (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:   5%|▌         | 1/20 [00:03<01:04,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:32,513] Trial 0 finished with value: 4.217164017071421 and parameters: {'n_estimators': 479, 'max_depth': 5, 'min_samples_split': 3, 'max_features': None}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  10%|█         | 2/20 [00:05<00:49,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:34,868] Trial 1 finished with value: 4.812751227214245 and parameters: {'n_estimators': 458, 'max_depth': 17, 'min_samples_split': 2, 'max_features': 'log2'}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  15%|█▌        | 3/20 [00:07<00:38,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:36,498] Trial 2 finished with value: 4.332436814091612 and parameters: {'n_estimators': 79, 'max_depth': 7, 'min_samples_split': 9, 'max_features': 0.5}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  20%|██        | 4/20 [00:09<00:35,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:38,695] Trial 3 finished with value: 4.980240867077739 and parameters: {'n_estimators': 453, 'max_depth': 5, 'min_samples_split': 7, 'max_features': 'sqrt'}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  25%|██▌       | 5/20 [00:10<00:24,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:39,209] Trial 4 finished with value: 5.8127476798319195 and parameters: {'n_estimators': 258, 'max_depth': 3, 'min_samples_split': 9, 'max_features': 'sqrt'}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  30%|███       | 6/20 [00:10<00:18,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:39,908] Trial 5 finished with value: 4.897576301576968 and parameters: {'n_estimators': 338, 'max_depth': 8, 'min_samples_split': 2, 'max_features': 'log2'}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  35%|███▌      | 7/20 [00:11<00:15,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:40,966] Trial 6 finished with value: 4.898853409582692 and parameters: {'n_estimators': 450, 'max_depth': 3, 'min_samples_split': 5, 'max_features': None}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  40%|████      | 8/20 [00:12<00:13,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:41,855] Trial 7 finished with value: 5.009796058862405 and parameters: {'n_estimators': 453, 'max_depth': 3, 'min_samples_split': 7, 'max_features': 0.5}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  45%|████▌     | 9/20 [00:12<00:09,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:42,088] Trial 8 finished with value: 5.01367662295862 and parameters: {'n_estimators': 67, 'max_depth': 11, 'min_samples_split': 9, 'max_features': 'log2'}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  50%|█████     | 10/20 [00:13<00:06,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:42,337] Trial 9 finished with value: 4.325379211583285 and parameters: {'n_estimators': 76, 'max_depth': 17, 'min_samples_split': 2, 'max_features': 0.5}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  55%|█████▌    | 11/20 [00:14<00:06,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:43,212] Trial 10 finished with value: 4.235991569296627 and parameters: {'n_estimators': 306, 'max_depth': 13, 'min_samples_split': 4, 'max_features': None}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  60%|██████    | 12/20 [00:15<00:06,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:44,176] Trial 11 finished with value: 4.235127818267906 and parameters: {'n_estimators': 308, 'max_depth': 13, 'min_samples_split': 4, 'max_features': None}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  65%|██████▌   | 13/20 [00:15<00:05,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:44,850] Trial 12 finished with value: 4.24357368131433 and parameters: {'n_estimators': 211, 'max_depth': 13, 'min_samples_split': 4, 'max_features': None}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 4.21716:  70%|███████   | 14/20 [00:16<00:05,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:45,940] Trial 13 finished with value: 4.239323554635833 and parameters: {'n_estimators': 365, 'max_depth': 20, 'min_samples_split': 4, 'max_features': None}. Best is trial 0 with value: 4.217164017071421.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 4.18403:  75%|███████▌  | 15/20 [00:17<00:03,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:46,541] Trial 14 finished with value: 4.184028665254133 and parameters: {'n_estimators': 167, 'max_depth': 10, 'min_samples_split': 3, 'max_features': None}. Best is trial 14 with value: 4.184028665254133.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 4.18403:  80%|████████  | 16/20 [00:17<00:02,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:47,071] Trial 15 finished with value: 4.242081076214215 and parameters: {'n_estimators': 160, 'max_depth': 9, 'min_samples_split': 3, 'max_features': None}. Best is trial 14 with value: 4.184028665254133.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 4.18403:  85%|████████▌ | 17/20 [00:18<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:47,707] Trial 16 finished with value: 4.2431747989145565 and parameters: {'n_estimators': 163, 'max_depth': 6, 'min_samples_split': 6, 'max_features': None}. Best is trial 14 with value: 4.184028665254133.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 4.18403:  90%|█████████ | 18/20 [00:19<00:01,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:48,883] Trial 17 finished with value: 4.208392489693545 and parameters: {'n_estimators': 393, 'max_depth': 10, 'min_samples_split': 3, 'max_features': None}. Best is trial 14 with value: 4.184028665254133.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 4.18403:  95%|█████████▌| 19/20 [00:20<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:49,694] Trial 18 finished with value: 4.94626463000239 and parameters: {'n_estimators': 395, 'max_depth': 10, 'min_samples_split': 6, 'max_features': 'sqrt'}. Best is trial 14 with value: 4.184028665254133.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 4.18403: 100%|██████████| 20/20 [00:21<00:00,  1.07s/it]\n",
      "[I 2025-12-03 15:46:50,466] A new study created in memory with name: no-name-3d7c8998-6bb8-4e69-a3b7-0f4f49fafeca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:50,460] Trial 19 finished with value: 4.2301917724544005 and parameters: {'n_estimators': 239, 'max_depth': 11, 'min_samples_split': 3, 'max_features': None}. Best is trial 14 with value: 4.184028665254133.\n",
      "RF best params: {'n_estimators': 167, 'max_depth': 10, 'min_samples_split': 3, 'max_features': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 11.7596:   5%|▌         | 1/20 [00:00<00:02,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[348]\tvalid_0's rmse: 11.7596\n",
      "[I 2025-12-03 15:46:50,615] Trial 0 finished with value: 11.759587628649983 and parameters: {'learning_rate': 0.014833210898008, 'num_leaves': 208, 'feature_fraction': 0.5842170642189065, 'bagging_fraction': 0.823132055174568}. Best is trial 0 with value: 11.759587628649983.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmse: 11.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 11.6226:  25%|██▌       | 5/20 [00:00<00:00, 15.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:50,664] Trial 1 finished with value: 11.691159878983198 and parameters: {'learning_rate': 0.12760544144808889, 'num_leaves': 95, 'feature_fraction': 0.9547247330066191, 'bagging_fraction': 0.9196261898092184}. Best is trial 1 with value: 11.691159878983198.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[101]\tvalid_0's rmse: 11.6314\n",
      "[I 2025-12-03 15:46:50,733] Trial 2 finished with value: 11.631408688108882 and parameters: {'learning_rate': 0.05000837757864231, 'num_leaves': 208, 'feature_fraction': 0.7649020397776838, 'bagging_fraction': 0.8787621717242314}. Best is trial 2 with value: 11.631408688108882.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's rmse: 12.4557\n",
      "[I 2025-12-03 15:46:50,766] Trial 3 finished with value: 12.455650295602114 and parameters: {'learning_rate': 0.18485893077405366, 'num_leaves': 216, 'feature_fraction': 0.5985166354333797, 'bagging_fraction': 0.5423234386317894}. Best is trial 2 with value: 11.631408688108882.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's rmse: 11.6226\n",
      "[I 2025-12-03 15:46:50,833] Trial 4 finished with value: 11.62256840050383 and parameters: {'learning_rate': 0.04374809057668837, 'num_leaves': 22, 'feature_fraction': 0.6564718245361895, 'bagging_fraction': 0.9698940307943432}. Best is trial 4 with value: 11.62256840050383.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's rmse: 11.9146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 11.6108:  40%|████      | 8/20 [00:00<00:00, 19.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:50,870] Trial 5 finished with value: 11.914584013878013 and parameters: {'learning_rate': 0.16306336761248597, 'num_leaves': 62, 'feature_fraction': 0.6354340300728443, 'bagging_fraction': 0.8448011587716066}. Best is trial 4 with value: 11.62256840050383.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid_0's rmse: 12.5411\n",
      "[I 2025-12-03 15:46:50,919] Trial 6 finished with value: 12.541085309021138 and parameters: {'learning_rate': 0.08655141487269348, 'num_leaves': 40, 'feature_fraction': 0.5116368899609258, 'bagging_fraction': 0.5500913438197688}. Best is trial 4 with value: 11.62256840050383.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's rmse: 12.5818\n",
      "[I 2025-12-03 15:46:50,950] Trial 7 finished with value: 12.581833053511803 and parameters: {'learning_rate': 0.17911071914641044, 'num_leaves': 47, 'feature_fraction': 0.5591792336884926, 'bagging_fraction': 0.5014353197813923}. Best is trial 4 with value: 11.62256840050383.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's rmse: 11.6108\n",
      "[I 2025-12-03 15:46:51,001] Trial 8 finished with value: 11.61083385860292 and parameters: {'learning_rate': 0.10389050670510942, 'num_leaves': 126, 'feature_fraction': 0.8698657524150426, 'bagging_fraction': 0.9980108603473792}. Best is trial 8 with value: 11.61083385860292.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[51]\tvalid_0's rmse: 11.6777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 11.6108:  50%|█████     | 10/20 [00:00<00:00, 19.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:51,053] Trial 9 finished with value: 11.67768112279497 and parameters: {'learning_rate': 0.09459377174886474, 'num_leaves': 236, 'feature_fraction': 0.8551808827957355, 'bagging_fraction': 0.8203530350626582}. Best is trial 8 with value: 11.61083385860292.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's rmse: 11.8236\n",
      "[I 2025-12-03 15:46:51,188] Trial 10 finished with value: 11.823629749489815 and parameters: {'learning_rate': 0.12175613100630886, 'num_leaves': 146, 'feature_fraction': 0.9812108509675936, 'bagging_fraction': 0.6798303811630049}. Best is trial 8 with value: 11.61083385860292.\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 11.6108:  60%|██████    | 12/20 [00:00<00:00, 13.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's rmse: 11.6616\n",
      "[I 2025-12-03 15:46:51,290] Trial 11 finished with value: 11.661575815011384 and parameters: {'learning_rate': 0.05850027986543628, 'num_leaves': 140, 'feature_fraction': 0.7178597024829546, 'bagging_fraction': 0.9959209852240273}. Best is trial 8 with value: 11.61083385860292.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[390]\tvalid_0's rmse: 11.6277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 11.6108:  70%|███████   | 14/20 [00:01<00:00, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:51,476] Trial 12 finished with value: 11.627665703336536 and parameters: {'learning_rate': 0.01389692483546474, 'num_leaves': 16, 'feature_fraction': 0.8598539296851507, 'bagging_fraction': 0.9624090182265651}. Best is trial 8 with value: 11.61083385860292.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's rmse: 11.8122\n",
      "[I 2025-12-03 15:46:51,576] Trial 13 finished with value: 11.81215116671338 and parameters: {'learning_rate': 0.05868682263397776, 'num_leaves': 109, 'feature_fraction': 0.7127557544218852, 'bagging_fraction': 0.724819321947803}. Best is trial 8 with value: 11.61083385860292.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's rmse: 11.615\n",
      "[I 2025-12-03 15:46:51,648] Trial 14 finished with value: 11.615030582421701 and parameters: {'learning_rate': 0.14744326393615814, 'num_leaves': 174, 'feature_fraction': 0.8060608318408653, 'bagging_fraction': 0.9355091148864788}. Best is trial 8 with value: 11.61083385860292.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 11.4957:  80%|████████  | 16/20 [00:01<00:00, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's rmse: 11.4957\n",
      "[I 2025-12-03 15:46:51,720] Trial 15 finished with value: 11.495708281441878 and parameters: {'learning_rate': 0.14671088813212468, 'num_leaves': 176, 'feature_fraction': 0.8471791083088034, 'bagging_fraction': 0.9222101588020202}. Best is trial 15 with value: 11.495708281441878.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's rmse: 11.8013\n",
      "[I 2025-12-03 15:46:51,795] Trial 16 finished with value: 11.801295652954671 and parameters: {'learning_rate': 0.13641877032403416, 'num_leaves': 174, 'feature_fraction': 0.8868667096791779, 'bagging_fraction': 0.7653193514670557}. Best is trial 15 with value: 11.495708281441878.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's rmse: 11.7311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 11.4527: 100%|██████████| 20/20 [00:01<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:46:51,881] Trial 17 finished with value: 11.7310892870008 and parameters: {'learning_rate': 0.11104510414637303, 'num_leaves': 103, 'feature_fraction': 0.9272929012110327, 'bagging_fraction': 0.9015194659964182}. Best is trial 15 with value: 11.495708281441878.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's rmse: 12.0967\n",
      "[I 2025-12-03 15:46:51,951] Trial 18 finished with value: 12.09666220425218 and parameters: {'learning_rate': 0.07921844455643015, 'num_leaves': 169, 'feature_fraction': 0.7860190053111616, 'bagging_fraction': 0.6355859676534005}. Best is trial 15 with value: 11.495708281441878.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's rmse: 11.4527\n",
      "[I 2025-12-03 15:46:52,025] Trial 19 finished with value: 11.452659667789959 and parameters: {'learning_rate': 0.1528668920931259, 'num_leaves': 124, 'feature_fraction': 0.822836806554386, 'bagging_fraction': 0.9970391811816629}. Best is trial 19 with value: 11.452659667789959.\n",
      "LightGBM best params: {'learning_rate': 0.1528668920931259, 'num_leaves': 124, 'feature_fraction': 0.822836806554386, 'bagging_fraction': 0.9970391811816629}\n",
      "RF tuned results: {'MAE': 1.3893184044904359, 'RMSE': 1.8029599254894189, 'R2': 0.9507947742308661}\n",
      "LightGBM tuned results: {'MAE': 1.6947070385193392, 'RMSE': 2.1589886322698897, 'R2': 0.9294430510428057}\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Neural Network results: {'MAE': 46.1820607623291, 'RMSE': 54.447128474172324, 'R2': -43.873406705565}\n",
      "Saved final evaluation results to outputs/final_evaluation_results.csv\n",
      "Best model by RMSE: linear_regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure: outputs\\figs\\shap_summary_linear_regression.png\n",
      "Saved SHAP feature importance for linear_regression\n",
      "Pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "run_full_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
