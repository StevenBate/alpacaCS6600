{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13fdc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import shap\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Helpers & output folders\n",
    "# -------------------------\n",
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUT_DIR, \"figs\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUT_DIR, \"models\"), exist_ok=True)\n",
    "\n",
    "def save_fig(name):\n",
    "    path = os.path.join(OUT_DIR, \"figs\", name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "    print(f\"Saved figure: {path}\")\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "615b19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Data loading: flexible\n",
    "# -------------------------\n",
    "def load_merged_df():\n",
    "    \"\"\"\n",
    "    Priority:\n",
    "      1) If the variable `merged_df` exists in the global namespace (e.g., run in notebook and import),\n",
    "         use that.\n",
    "      2) Else attempt to load './merged_df.csv' (should be exported from your EDA).\n",
    "      3) Else, raise an informative error and show the Alpaca fetch snippet you can enable.\n",
    "    \"\"\"\n",
    "    csv_path = \"data.csv\"\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path, parse_dates=['timestamp', 'date'], infer_datetime_format=True)\n",
    "        return df\n",
    "    # 3. error + instructions\n",
    "    raise FileNotFoundError(\n",
    "        \"No merged_df in globals and './merged_df.csv' not found.\\n\"\n",
    "        \"If you want to fetch from Alpaca directly, uncomment and fill the Alpaca block in this file.\\n\"\n",
    "        \"Alternatively export your merged DataFrame to ./merged_df.csv and re-run.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99dd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_engineer(df, debug=False):\n",
    "    \"\"\"\n",
    "    Input: merged_df (as produced by your EDA)\n",
    "    Steps:\n",
    "      - Ensure timestamp/date columns\n",
    "      - Create target = next-day high\n",
    "      - Lag features, rolling stats\n",
    "      - Handle missing values\n",
    "      - Standardize features (scaler fitted on training set later)\n",
    "    Returns: processed dataframe (no scaling applied), feature list\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # ensure datetime\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    if 'date' in df.columns and not np.issubdtype(df['date'].dtype, np.datetime64):\n",
    "        try:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "        except:\n",
    "            df['date'] = pd.to_datetime(df['timestamp'].dt.date)\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['timestamp'].dt.date)\n",
    "\n",
    "    # Sort by time\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Target: tomorrow's high\n",
    "    df['target_high'] = df['high'].shift(-1)\n",
    "\n",
    "    # lag features\n",
    "    lags = [1,2,3]\n",
    "    for lag in lags:\n",
    "        df[f'close_lag_{lag}'] = df['close'].shift(lag)\n",
    "        df[f'return_lag_{lag}'] = df['daily_return'].shift(lag)\n",
    "        df[f'vol_lag_{lag}'] = df['volume'].shift(lag)\n",
    "\n",
    "    # Rolling stats\n",
    "    df['rolling_mean_5'] = df['close'].rolling(5).mean()\n",
    "    df['rolling_std_5'] = df['close'].rolling(5).std()\n",
    "    df['rolling_mean_10'] = df['close'].rolling(10).mean()\n",
    "    df['rolling_std_10'] = df['close'].rolling(10).std()\n",
    "\n",
    "    # Sentiment lags\n",
    "    df['sentiment_lag_1'] = df['avg_sentiment'].shift(1)\n",
    "\n",
    "    # Time features\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "    # Drop rows with NA in target\n",
    "    df = df.dropna(subset=['target_high']).reset_index(drop=True)\n",
    "\n",
    "    # Fill remaining missing feature values with median (simple imputer later)\n",
    "    # Keep columns list\n",
    "    feature_cols = [\n",
    "        'open','high','low','close','volume','vwap','trade_count','daily_return',\n",
    "        'rolling_mean_5','rolling_std_5','rolling_mean_10','rolling_std_10',\n",
    "        'close_lag_1','close_lag_2','close_lag_3',\n",
    "        'return_lag_1','return_lag_2','return_lag_3',\n",
    "        'vol_lag_1','vol_lag_2','vol_lag_3',\n",
    "        'sentiment_lag_1','dayofweek','month'\n",
    "    ]\n",
    "    # Some columns on user data may not exist (e.g., trade_count)\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "    if debug:\n",
    "        print(\"Feature columns:\", feature_cols)\n",
    "\n",
    "    # We'll not scale here; scaling is part of a sklearn pipeline fitted on train.\n",
    "    return df, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c195c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_only_eda(X_train_df, y_train, out_prefix=\"train\"):\n",
    "    \"\"\"\n",
    "    Perform exploration ONLY on the training set per assignment.\n",
    "    Save descriptive stats, correlation matrix, and plots.\n",
    "    \"\"\"\n",
    "    print(\"=== Training-only EDA ===\")\n",
    "    stats = X_train_df.describe().T\n",
    "    stats.to_csv(os.path.join(OUT_DIR, f\"{out_prefix}_descriptive_stats.csv\"))\n",
    "    print(f\"Saved descriptive stats to {OUT_DIR}/{out_prefix}_descriptive_stats.csv\")\n",
    "\n",
    "    # Correlation (features + target)\n",
    "    corr = X_train_df.join(y_train).corr()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(corr, annot=False, cmap='coolwarm')\n",
    "    plt.title('Training-set Correlation Matrix (features + target)')\n",
    "    save_fig(f\"{out_prefix}_corr_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Distribution plots for a handful of numeric features\n",
    "    cols = X_train_df.select_dtypes(include=[np.number]).columns.tolist()[:8]\n",
    "    for c in cols:\n",
    "        plt.figure(figsize=(5,3))\n",
    "        sns.histplot(X_train_df[c].dropna(), kde=True, bins=30)\n",
    "        plt.title(f'{c} distribution (train)')\n",
    "        save_fig(f\"{out_prefix}_dist_{c}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Scatter: sentiment_lag_1 vs target (if present)\n",
    "    if 'sentiment_lag_1' in X_train_df.columns:\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.scatterplot(x=X_train_df['sentiment_lag_1'], y=y_train)\n",
    "        plt.title('Sentiment (lag1) vs Target High (train)')\n",
    "        save_fig(f\"{out_prefix}_sentiment_vs_target.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4f0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(y_true, y_pred):\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': rmse(y_true, y_pred),\n",
    "        'R2': r2_score(y_true, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e30a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train baseline, LinearRegression, RandomForest, LightGBM, (optional) Keras.\n",
    "    Return fitted models and metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    models = {}\n",
    "\n",
    "    baseline_pred = X_test['high'].values\n",
    "    results['baseline'] = evaluate_regression(y_test, baseline_pred)\n",
    "    print(\"Baseline:\", results['baseline'])\n",
    "\n",
    "    # Pipeline to impute + scale\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    X_test_scaled = scaler.transform(imputer.transform(X_test))\n",
    "\n",
    "    # Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    lr_pred = lr.predict(X_test_scaled)\n",
    "    results['linear_regression'] = evaluate_regression(y_test, lr_pred)\n",
    "    models['linear_regression'] = ('lr', lr, imputer, scaler)\n",
    "    print(\"Linear Regression:\", results['linear_regression'])\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestRegressor(n_estimators=300, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    rf_pred = rf.predict(X_test_scaled)\n",
    "    results['random_forest'] = evaluate_regression(y_test, rf_pred)\n",
    "    models['random_forest'] = ('rf', rf, imputer, scaler)\n",
    "    print(\"Random Forest:\", results['random_forest'])\n",
    "\n",
    "    # LightGBM\n",
    "    lgbm = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.05, random_state=42)\n",
    "    lgbm.fit(X_train_scaled, y_train)\n",
    "    lgbm_pred = lgbm.predict(X_test_scaled)\n",
    "    results['lightgbm'] = evaluate_regression(y_test, lgbm_pred)\n",
    "    models['lightgbm'] = ('lgbm', lgbm, imputer, scaler)\n",
    "    print(\"LightGBM:\", results['lightgbm'])\n",
    "\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    nn = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # regression output\n",
    "    ])\n",
    "    nn.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=[])\n",
    "\n",
    "    nn.fit(X_train_scaled, y_train, validation_split=0.1, epochs=50, batch_size=32, verbose=0)\n",
    "    nn_pred = nn.predict(X_test_scaled).flatten()\n",
    "    results['keras_nn'] = evaluate_regression(y_test, nn_pred)\n",
    "    models['keras_nn'] = ('keras_nn', nn, imputer, scaler)\n",
    "    print(\"Keras NN results:\", results['keras_nn'])\n",
    "\n",
    "    # Save models\n",
    "    for name, tup in models.items():\n",
    "        tag, m, imputer_obj, scaler_obj = tup\n",
    "        joblib.dump(tup, os.path.join(OUT_DIR, \"models\", f\"{name}.joblib\"))\n",
    "    # also save results\n",
    "    res_df = pd.DataFrame(results).T\n",
    "    res_df.to_csv(os.path.join(OUT_DIR, \"model_results.csv\"))\n",
    "    print(\"Saved model results to outputs/model_results.csv\")\n",
    "    return models, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53eda397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_analysis(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Run KMeans, PCA, Agglomerative, and IsolationForest\n",
    "    \"\"\"\n",
    "    print(\"=== Unsupervised analysis ===\")\n",
    "    X_unsup = df[feature_cols].select_dtypes(include=[np.number]).fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X_unsup)\n",
    "\n",
    "    # PCA (2 components for visualization)\n",
    "    pca = PCA(n_components=2, random_state=0)\n",
    "    pcs = pca.fit_transform(Xs)\n",
    "    pca_df = pd.DataFrame(pcs, columns=['PC1','PC2'])\n",
    "    pca_df.to_csv(os.path.join(OUT_DIR, \"pca_components.csv\"))\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'])\n",
    "    plt.title('PCA (2 components)')\n",
    "    save_fig(\"unsup_pca.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # KMeans\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "    labels = kmeans.fit_predict(Xs)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=labels, palette='tab10')\n",
    "    plt.title('KMeans clusters (k=3) on PCA')\n",
    "    save_fig(\"unsup_kmeans_pca.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Agglomerative\n",
    "    agg = AgglomerativeClustering(n_clusters=3)\n",
    "    agg_labels = agg.fit_predict(Xs)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=agg_labels, palette='deep')\n",
    "    plt.title('Agglomerative clusters on PCA')\n",
    "    save_fig(\"unsup_agg_pca.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # IsolationForest for anomalies\n",
    "    iso = IsolationForest(random_state=0)\n",
    "    iso_pred = iso.fit_predict(Xs)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=iso_pred, palette='Set1')\n",
    "    plt.title('IsolationForest (anomaly detection)')\n",
    "    save_fig(\"unsup_iso_pca.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save clustering models\n",
    "    joblib.dump((kmeans, scaler), os.path.join(OUT_DIR, \"models\", \"kmeans.joblib\"))\n",
    "    joblib.dump((agg, scaler), os.path.join(OUT_DIR, \"models\", \"agg.joblib\"))\n",
    "    joblib.dump((iso, scaler), os.path.join(OUT_DIR, \"models\", \"iso.joblib\"))\n",
    "\n",
    "    return {\n",
    "        'pca': pca,\n",
    "        'kmeans': kmeans,\n",
    "        'agg': agg,\n",
    "        'isolation_forest': iso\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55639355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_validation(model, X, y, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    scores = []\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_tr, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        # simple pipeline: impute -> scale -> fit model\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_scaled = scaler.fit_transform(imputer.fit_transform(X_tr))\n",
    "        X_val_scaled = scaler.transform(imputer.transform(X_val))\n",
    "        model.fit(X_tr_scaled, y_tr)\n",
    "        pred = model.predict(X_val_scaled)\n",
    "        scores.append(rmse(y_val, pred))\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b050d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_tune_rf(X_train, y_train, n_trials=30):\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "        max_features = trial.suggest_categorical('max_features', ['sqrt','log2', 0.5, None])\n",
    "\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        # cross-validate using simple 3-fold (not time series CV for speed)\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "        scores = -cross_val_score(model, Xs, y_train, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    print(\"RF best params:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e883fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keras_nn(X_train, y_train, X_test, y_test, \n",
    "                   hidden_layers=[64, 32], dropout=0.2, \n",
    "                   lr=0.001, batch_size=32, epochs=200, patience=20):\n",
    "    \"\"\"\n",
    "    Train a simple feedforward neural network for regression.\n",
    "    \n",
    "    Returns:\n",
    "        model: trained Keras model\n",
    "        results: dict of metrics on test set\n",
    "    \"\"\"\n",
    "    # Impute and scale\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "    Xte = scaler.transform(imputer.transform(X_test))\n",
    "    \n",
    "    # Build model\n",
    "    model = Sequential()\n",
    "    input_dim = Xtr.shape[1]\n",
    "    for i, units in enumerate(hidden_layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(units, activation='relu', input_dim=input_dim))\n",
    "        else:\n",
    "            model.add(Dense(units, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='linear'))  # regression output\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        Xtr, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[es],\n",
    "        verbose=0  # change to 1 to see progress\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(Xte).flatten()\n",
    "    results = evaluate_regression(y_test, y_pred)\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(('keras_nn', model, imputer, scaler), os.path.join(OUT_DIR, \"models\", \"keras_nn.joblib\"))\n",
    "    print(\"Neural Network results:\", results)\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dcb80db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_tune_lgb(X_train, y_train, n_trials=30):\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "            'bagging_freq': 1\n",
    "        }\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "        split_idx = int(0.8 * len(Xs))\n",
    "        X_tr, X_val = Xs[:split_idx], Xs[split_idx:]\n",
    "        y_tr, y_val = y_train.iloc[:split_idx], y_train.iloc[split_idx:]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
    "        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "\n",
    "        # Train with early stopping\n",
    "        cvres = lgb.train(\n",
    "            params=param,\n",
    "            train_set=dtrain,\n",
    "            valid_sets=[dval],\n",
    "            num_boost_round=1000,\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=20),   # early stopping\n",
    "                lgb.log_evaluation(period=0)              # suppress printing\n",
    "            ]\n",
    "        )\n",
    "        return cvres.best_score['valid_0']['rmse']\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    print(\"LightGBM best params:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a63da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_explain(model_tuple, X_train, X_test, feature_names, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    model_tuple: (tag, model, imputer, scaler) saved earlier\n",
    "    \"\"\"\n",
    "    tag, model, imputer, scaler = model_tuple\n",
    "    # preprocess\n",
    "    X_train_pre = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "    X_test_pre = scaler.transform(imputer.transform(X_test))\n",
    "\n",
    "    # SHAP for tree models (RandomForest, LightGBM)\n",
    "    model_type = str(type(model)).lower()\n",
    "    explainer = None\n",
    "    if hasattr(model, 'predict'):\n",
    "        try:\n",
    "            if 'lgbm' in model_type or 'lightgbm' in model_type:\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "            elif 'randomforest' in model_type or 'forest' in model_type:\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "            elif 'sequential' in model_type or 'keras' in model_type:\n",
    "                background = X_train_pre[np.random.choice(X_train_pre.shape[0], min(100, X_train_pre.shape[0]), replace=False)]\n",
    "                explainer = shap.KernelExplainer(model.predict, background)\n",
    "                shap_values = explainer.shap_values(X_test_pre[:50], nsamples=100)\n",
    "            else:\n",
    "                explainer = shap.KernelExplainer(model.predict, X_train_pre[:100])\n",
    "        except Exception as e:\n",
    "            print(\"SHAP explainer construction failed:\", e)\n",
    "            return\n",
    "    else:\n",
    "        print(\"Model object not suitable for SHAP\")\n",
    "        return\n",
    "\n",
    "    shap_values = explainer.shap_values(X_test_pre[:200])  # limit for speed\n",
    "    # summary plot\n",
    "    plt.figure(figsize=(6,4))\n",
    "    shap.summary_plot(shap_values, X_test_pre[:200], feature_names=feature_names, show=False)\n",
    "    save_fig(f\"shap_summary_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # feature importance as table\n",
    "    mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "    fi = pd.Series(mean_abs, index=feature_names).sort_values(ascending=False)\n",
    "    fi.to_csv(os.path.join(OUT_DIR, f\"shap_feature_importance_{model_name}.csv\"))\n",
    "    print(f\"Saved SHAP feature importance for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54e3af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline():\n",
    "    df = load_merged_df()\n",
    "    df, feature_cols = preprocess_and_engineer(df, debug=True)\n",
    "\n",
    "    # train/test split: use temporal split (no shuffle)\n",
    "    n = len(df)\n",
    "    test_size = int(0.2 * n)\n",
    "    train_df = df.iloc[:-test_size].reset_index(drop=True)\n",
    "    test_df = df.iloc[-test_size:].reset_index(drop=True)\n",
    "\n",
    "    # Training-only EDA\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df['target_high']\n",
    "    training_only_eda(X_train, y_train, out_prefix=\"train\")\n",
    "\n",
    "    # Unsupervised analysis\n",
    "    unsup_models = unsupervised_analysis(train_df, feature_cols)\n",
    "\n",
    "    # Prepare test set\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df['target_high']\n",
    "\n",
    "    # Train supervised models\n",
    "    models, results = train_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # TimeSeries CV for naive RF\n",
    "    try:\n",
    "        mean_rmse, std_rmse = time_series_validation(\n",
    "            RandomForestRegressor(n_estimators=100),\n",
    "            train_df[feature_cols],\n",
    "            train_df['target_high'],\n",
    "            n_splits=5\n",
    "        )\n",
    "        print(f\"TimeSeriesCV (naive RF) RMSE: mean {mean_rmse:.4f}, std {std_rmse:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"TimeSeries CV failed:\", e)\n",
    "\n",
    "    # Hyperparameter tuning with Optuna\n",
    "    print(\"Starting Optuna tuning (this may take a while)...\")\n",
    "    try:\n",
    "        best_rf_params = optuna_tune_rf(X_train, y_train, n_trials=20)\n",
    "        best_lgb_params = optuna_tune_lgb(X_train, y_train, n_trials=20)\n",
    "    except Exception as e:\n",
    "        print(\"Optuna tuning failed or interrupted:\", e)\n",
    "        best_rf_params, best_lgb_params = None, None\n",
    "\n",
    "    # Fit tuned models\n",
    "    tuned_results = {}\n",
    "    if best_rf_params:\n",
    "        rf_tuned = RandomForestRegressor(**best_rf_params, random_state=42, n_jobs=-1)\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "        rf_tuned.fit(Xtr, y_train)\n",
    "        Xte = scaler.transform(imputer.transform(X_test))\n",
    "        rf_pred = rf_tuned.predict(Xte)\n",
    "        tuned_results['rf_tuned'] = evaluate_regression(y_test, rf_pred)\n",
    "        joblib.dump(('rf_tuned', rf_tuned, imputer, scaler), os.path.join(OUT_DIR, \"models\", \"rf_tuned.joblib\"))\n",
    "        print(\"RF tuned results:\", tuned_results['rf_tuned'])\n",
    "    if best_lgb_params:\n",
    "        lgb_tuned = lgb.LGBMRegressor(**best_lgb_params, random_state=42)\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(imputer.fit_transform(X_train))\n",
    "        lgb_tuned.fit(Xtr, y_train)\n",
    "        Xte = scaler.transform(imputer.transform(X_test))\n",
    "        lgb_pred = lgb_tuned.predict(Xte)\n",
    "        tuned_results['lgb_tuned'] = evaluate_regression(y_test, lgb_pred)\n",
    "        joblib.dump(('lgb_tuned', lgb_tuned, imputer, scaler), os.path.join(OUT_DIR, \"models\", \"lgb_tuned.joblib\"))\n",
    "        print(\"LightGBM tuned results:\", tuned_results['lgb_tuned'])\n",
    "\n",
    "    # --- Neural Network ---\n",
    "    try:\n",
    "        nn_model, nn_results = train_keras_nn(X_train, y_train, X_test, y_test)\n",
    "        tuned_results['keras_nn'] = nn_results\n",
    "    except Exception as e:\n",
    "        print(\"Neural network training failed:\", e)\n",
    "\n",
    "    # Final evaluation: combine all results\n",
    "    all_results = []\n",
    "    for name, metrics in results.items():\n",
    "        row = {'model': name}\n",
    "        row.update(metrics)\n",
    "        all_results.append(row)\n",
    "    for name, metrics in tuned_results.items():\n",
    "        row = {'model': name}\n",
    "        row.update(metrics)\n",
    "        all_results.append(row)\n",
    "\n",
    "    res_df = pd.DataFrame(all_results).set_index('model')\n",
    "    res_df.to_csv(os.path.join(OUT_DIR, \"final_evaluation_results.csv\"))\n",
    "    print(\"Saved final evaluation results to outputs/final_evaluation_results.csv\")\n",
    "\n",
    "    if not res_df.empty:\n",
    "        best_model_name = res_df['RMSE'].idxmin()\n",
    "        print(\"Best model by RMSE:\", best_model_name)\n",
    "        model_tuple = None\n",
    "        # Try to load the model object\n",
    "        try:\n",
    "            model_tuple = joblib.load(os.path.join(OUT_DIR, \"models\", f\"{best_model_name}.joblib\"))\n",
    "        except Exception:\n",
    "            try:\n",
    "                model_tuple = joblib.load(os.path.join(OUT_DIR, \"models\", f\"{best_model_name}_tuned.joblib\"))\n",
    "            except Exception:\n",
    "                if best_model_name in models:\n",
    "                    model_tuple = models[best_model_name]\n",
    "        if model_tuple:\n",
    "            try:\n",
    "                shap_explain(model_tuple, X_train, X_test, feature_cols, model_name=best_model_name)\n",
    "            except Exception as e:\n",
    "                print(\"SHAP explanation failed:\", e)\n",
    "        else:\n",
    "            print(\"Could not find model object for SHAP explanation:\", best_model_name)\n",
    "    print(\"Pipeline complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e05a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: ['open', 'high', 'low', 'close', 'volume', 'vwap', 'trade_count', 'daily_return', 'rolling_mean_5', 'rolling_std_5', 'rolling_mean_10', 'rolling_std_10', 'close_lag_1', 'close_lag_2', 'close_lag_3', 'return_lag_1', 'return_lag_2', 'return_lag_3', 'vol_lag_1', 'vol_lag_2', 'vol_lag_3', 'sentiment_lag_1', 'dayofweek', 'month']\n",
      "=== Training-only EDA ===\n",
      "Saved descriptive stats to outputs/train_descriptive_stats.csv\n",
      "Saved figure: outputs\\figs\\train_corr_matrix.png\n",
      "Saved figure: outputs\\figs\\train_dist_open.png\n",
      "Saved figure: outputs\\figs\\train_dist_high.png\n",
      "Saved figure: outputs\\figs\\train_dist_low.png\n",
      "Saved figure: outputs\\figs\\train_dist_close.png\n",
      "Saved figure: outputs\\figs\\train_dist_volume.png\n",
      "Saved figure: outputs\\figs\\train_dist_vwap.png\n",
      "Saved figure: outputs\\figs\\train_dist_trade_count.png\n",
      "Saved figure: outputs\\figs\\train_dist_daily_return.png\n",
      "Saved figure: outputs\\figs\\train_sentiment_vs_target.png\n",
      "=== Unsupervised analysis ===\n",
      "Saved figure: outputs\\figs\\unsup_pca.png\n",
      "Saved figure: outputs\\figs\\unsup_kmeans_pca.png\n",
      "Saved figure: outputs\\figs\\unsup_agg_pca.png\n",
      "Saved figure: outputs\\figs\\unsup_iso_pca.png\n",
      "Baseline: {'MAE': 1.4658800000000007, 'RMSE': 4.034853648399998, 'R2': 0.9389245229252972}\n",
      "Linear Regression: {'MAE': 1.2057438899065198, 'RMSE': 2.465277015270269, 'R2': 0.9626831645086711}\n",
      "Random Forest: {'MAE': 1.384129579120432, 'RMSE': 3.244310346139128, 'R2': 0.9508909567891226}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2963\n",
      "[LightGBM] [Info] Number of data points in the train set: 400, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 160.798176\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM: {'MAE': 1.6431832595526772, 'RMSE': 4.327204279067481, 'R2': 0.9344992188629831}\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Keras NN results: {'MAE': 53.460472037841804, 'RMSE': 4666.678510763929, 'R2': -69.63939394981378}\n",
      "Saved model results to outputs/model_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:23:35,290] A new study created in memory with name: no-name-ca841e68-04ea-4f03-a7b7-057df941877b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesCV (naive RF) RMSE: mean 54.8681, std 60.0133\n",
      "Starting Optuna tuning (this may take a while)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e665f7dcdd7f46c395b76fd6d413b8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:23:39,438] Trial 0 finished with value: 4.262026684325402 and parameters: {'n_estimators': 357, 'max_depth': 14, 'min_samples_split': 7, 'max_features': None}. Best is trial 0 with value: 4.262026684325402.\n",
      "[I 2025-12-03 15:23:42,552] Trial 1 finished with value: 4.272379867633962 and parameters: {'n_estimators': 254, 'max_depth': 5, 'min_samples_split': 2, 'max_features': None}. Best is trial 0 with value: 4.262026684325402.\n",
      "[I 2025-12-03 15:23:45,845] Trial 2 finished with value: 4.248195515991469 and parameters: {'n_estimators': 426, 'max_depth': 4, 'min_samples_split': 4, 'max_features': None}. Best is trial 2 with value: 4.248195515991469.\n",
      "[I 2025-12-03 15:23:48,999] Trial 3 finished with value: 4.8925696856590095 and parameters: {'n_estimators': 303, 'max_depth': 3, 'min_samples_split': 4, 'max_features': None}. Best is trial 2 with value: 4.248195515991469.\n",
      "[I 2025-12-03 15:23:49,233] Trial 4 finished with value: 5.015900511149044 and parameters: {'n_estimators': 106, 'max_depth': 12, 'min_samples_split': 10, 'max_features': 'sqrt'}. Best is trial 2 with value: 4.248195515991469.\n",
      "[I 2025-12-03 15:23:49,681] Trial 5 finished with value: 4.252853709353709 and parameters: {'n_estimators': 176, 'max_depth': 10, 'min_samples_split': 7, 'max_features': None}. Best is trial 2 with value: 4.248195515991469.\n",
      "[I 2025-12-03 15:23:50,460] Trial 6 finished with value: 4.310801504960771 and parameters: {'n_estimators': 337, 'max_depth': 12, 'min_samples_split': 7, 'max_features': 0.5}. Best is trial 2 with value: 4.248195515991469.\n",
      "[I 2025-12-03 15:23:51,296] Trial 7 finished with value: 4.314358225646787 and parameters: {'n_estimators': 422, 'max_depth': 18, 'min_samples_split': 9, 'max_features': 0.5}. Best is trial 2 with value: 4.248195515991469.\n",
      "[I 2025-12-03 15:23:51,813] Trial 8 finished with value: 5.011753189047067 and parameters: {'n_estimators': 289, 'max_depth': 3, 'min_samples_split': 9, 'max_features': 0.5}. Best is trial 2 with value: 4.248195515991469.\n",
      "[I 2025-12-03 15:23:52,950] Trial 9 finished with value: 4.183094281578352 and parameters: {'n_estimators': 488, 'max_depth': 6, 'min_samples_split': 2, 'max_features': None}. Best is trial 9 with value: 4.183094281578352.\n",
      "[I 2025-12-03 15:23:53,854] Trial 10 finished with value: 4.891779611281824 and parameters: {'n_estimators': 496, 'max_depth': 8, 'min_samples_split': 2, 'max_features': 'log2'}. Best is trial 9 with value: 4.183094281578352.\n",
      "[I 2025-12-03 15:23:55,010] Trial 11 finished with value: 4.216776398860851 and parameters: {'n_estimators': 493, 'max_depth': 7, 'min_samples_split': 4, 'max_features': None}. Best is trial 9 with value: 4.183094281578352.\n",
      "[I 2025-12-03 15:23:55,887] Trial 12 finished with value: 4.908057314318162 and parameters: {'n_estimators': 500, 'max_depth': 7, 'min_samples_split': 4, 'max_features': 'sqrt'}. Best is trial 9 with value: 4.183094281578352.\n",
      "[I 2025-12-03 15:23:56,636] Trial 13 finished with value: 4.889403696013598 and parameters: {'n_estimators': 417, 'max_depth': 8, 'min_samples_split': 3, 'max_features': 'log2'}. Best is trial 9 with value: 4.183094281578352.\n",
      "[I 2025-12-03 15:23:57,650] Trial 14 finished with value: 4.235868755614773 and parameters: {'n_estimators': 445, 'max_depth': 6, 'min_samples_split': 5, 'max_features': None}. Best is trial 9 with value: 4.183094281578352.\n",
      "[I 2025-12-03 15:23:58,698] Trial 15 finished with value: 4.2357673817840995 and parameters: {'n_estimators': 373, 'max_depth': 10, 'min_samples_split': 2, 'max_features': None}. Best is trial 9 with value: 4.183094281578352.\n",
      "[I 2025-12-03 15:23:59,315] Trial 16 finished with value: 4.265057794842014 and parameters: {'n_estimators': 224, 'max_depth': 15, 'min_samples_split': 5, 'max_features': None}. Best is trial 9 with value: 4.183094281578352.\n",
      "[I 2025-12-03 15:24:00,169] Trial 17 finished with value: 4.875982695929033 and parameters: {'n_estimators': 467, 'max_depth': 9, 'min_samples_split': 3, 'max_features': 'log2'}. Best is trial 9 with value: 4.183094281578352.\n",
      "[I 2025-12-03 15:24:00,962] Trial 18 finished with value: 4.898451226682426 and parameters: {'n_estimators': 388, 'max_depth': 6, 'min_samples_split': 3, 'max_features': 'sqrt'}. Best is trial 9 with value: 4.183094281578352.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:24:01,263] A new study created in memory with name: no-name-ed5d2a1f-9e41-455e-8f75-063c25911c65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:24:01,260] Trial 19 finished with value: 4.198411291515193 and parameters: {'n_estimators': 100, 'max_depth': 18, 'min_samples_split': 6, 'max_features': None}. Best is trial 9 with value: 4.183094281578352.\n",
      "RF best params: {'n_estimators': 488, 'max_depth': 6, 'min_samples_split': 2, 'max_features': None}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac230e01cf44604adfa151241ee5808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's rmse: 11.6182\n",
      "[I 2025-12-03 15:24:01,406] Trial 0 finished with value: 11.618163715895006 and parameters: {'learning_rate': 0.03292853586877209, 'num_leaves': 122, 'feature_fraction': 0.9089512392803559, 'bagging_fraction': 0.7980019243561534}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's rmse: 11.9264\n",
      "[I 2025-12-03 15:24:01,471] Trial 1 finished with value: 11.926428249769433 and parameters: {'learning_rate': 0.047444188883595376, 'num_leaves': 164, 'feature_fraction': 0.5668812889377421, 'bagging_fraction': 0.7038138609221432}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's rmse: 11.8599\n",
      "[I 2025-12-03 15:24:01,513] Trial 2 finished with value: 11.859882892482277 and parameters: {'learning_rate': 0.1504404919995885, 'num_leaves': 151, 'feature_fraction': 0.8231234336020614, 'bagging_fraction': 0.7005489409821644}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's rmse: 11.7966\n",
      "[I 2025-12-03 15:24:01,618] Trial 3 finished with value: 11.796639949606167 and parameters: {'learning_rate': 0.031000283607773238, 'num_leaves': 127, 'feature_fraction': 0.9397187408081185, 'bagging_fraction': 0.6991397377624562}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid_0's rmse: 11.8127\n",
      "[I 2025-12-03 15:24:01,662] Trial 4 finished with value: 11.812741475846323 and parameters: {'learning_rate': 0.07814167419938632, 'num_leaves': 208, 'feature_fraction': 0.9838888457631287, 'bagging_fraction': 0.5942628449857162}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[492]\tvalid_0's rmse: 12.3364\n",
      "[I 2025-12-03 15:24:01,840] Trial 5 finished with value: 12.336415753766518 and parameters: {'learning_rate': 0.01157160179498281, 'num_leaves': 153, 'feature_fraction': 0.5733588031814796, 'bagging_fraction': 0.5160363522565395}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's rmse: 12.3547\n",
      "[I 2025-12-03 15:24:01,872] Trial 6 finished with value: 12.354658543419633 and parameters: {'learning_rate': 0.1509783648468882, 'num_leaves': 124, 'feature_fraction': 0.5068681139220448, 'bagging_fraction': 0.5619209000360719}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's rmse: 11.8928\n",
      "[I 2025-12-03 15:24:01,974] Trial 7 finished with value: 11.892825415806197 and parameters: {'learning_rate': 0.07467253309637831, 'num_leaves': 118, 'feature_fraction': 0.5564659215001948, 'bagging_fraction': 0.7805054971624579}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's rmse: 12.0779\n",
      "[I 2025-12-03 15:24:02,011] Trial 8 finished with value: 12.077891047783536 and parameters: {'learning_rate': 0.174494535082779, 'num_leaves': 173, 'feature_fraction': 0.83898351737883, 'bagging_fraction': 0.6054129024448915}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's rmse: 11.645\n",
      "[I 2025-12-03 15:24:02,065] Trial 9 finished with value: 11.645020237011911 and parameters: {'learning_rate': 0.11300833732538693, 'num_leaves': 91, 'feature_fraction': 0.8234656189774396, 'bagging_fraction': 0.8786490047962585}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's rmse: 11.8763\n",
      "[I 2025-12-03 15:24:02,120] Trial 10 finished with value: 11.876313457510399 and parameters: {'learning_rate': 0.11246404735507617, 'num_leaves': 39, 'feature_fraction': 0.6858097687688021, 'bagging_fraction': 0.9540848617845923}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's rmse: 11.6432\n",
      "[I 2025-12-03 15:24:02,187] Trial 11 finished with value: 11.643213828589888 and parameters: {'learning_rate': 0.11875261898966898, 'num_leaves': 61, 'feature_fraction': 0.8841664103559569, 'bagging_fraction': 0.868726747690066}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's rmse: 11.6195\n",
      "[I 2025-12-03 15:24:02,244] Trial 12 finished with value: 11.61952545894709 and parameters: {'learning_rate': 0.07271705356669776, 'num_leaves': 56, 'feature_fraction': 0.8991935767708911, 'bagging_fraction': 0.8259949955123802}. Best is trial 0 with value: 11.618163715895006.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's rmse: 11.6092\n",
      "[I 2025-12-03 15:24:02,337] Trial 13 finished with value: 11.609206918890969 and parameters: {'learning_rate': 0.06262146074968952, 'num_leaves': 20, 'feature_fraction': 0.71949020633809, 'bagging_fraction': 0.8082009826745642}. Best is trial 13 with value: 11.609206918890969.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's rmse: 11.6608\n",
      "[I 2025-12-03 15:24:02,446] Trial 14 finished with value: 11.660831824581194 and parameters: {'learning_rate': 0.0407518945839343, 'num_leaves': 254, 'feature_fraction': 0.7229231301107382, 'bagging_fraction': 0.9348577412727627}. Best is trial 13 with value: 11.609206918890969.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[442]\tvalid_0's rmse: 11.764\n",
      "[I 2025-12-03 15:24:02,706] Trial 15 finished with value: 11.76404881274394 and parameters: {'learning_rate': 0.011679454798288247, 'num_leaves': 17, 'feature_fraction': 0.6657921152182141, 'bagging_fraction': 0.7875272699031585}. Best is trial 13 with value: 11.609206918890969.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's rmse: 11.8027\n",
      "[I 2025-12-03 15:24:02,767] Trial 16 finished with value: 11.802666239637805 and parameters: {'learning_rate': 0.05811158127372053, 'num_leaves': 89, 'feature_fraction': 0.7604574655038236, 'bagging_fraction': 0.9994927828742121}. Best is trial 13 with value: 11.609206918890969.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[51]\tvalid_0's rmse: 11.9384\n",
      "[I 2025-12-03 15:24:02,853] Trial 17 finished with value: 11.93840759364524 and parameters: {'learning_rate': 0.09491319604264234, 'num_leaves': 196, 'feature_fraction': 0.7683634145445167, 'bagging_fraction': 0.7399943833726743}. Best is trial 13 with value: 11.609206918890969.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[193]\tvalid_0's rmse: 11.7097\n",
      "[I 2025-12-03 15:24:03,017] Trial 18 finished with value: 11.709653402235768 and parameters: {'learning_rate': 0.03024451003506512, 'num_leaves': 86, 'feature_fraction': 0.6729089004740897, 'bagging_fraction': 0.8466504801280554}. Best is trial 13 with value: 11.609206918890969.\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's rmse: 11.8066\n",
      "[I 2025-12-03 15:24:03,110] Trial 19 finished with value: 11.806564076856185 and parameters: {'learning_rate': 0.06259328326342023, 'num_leaves': 16, 'feature_fraction': 0.6199631856220508, 'bagging_fraction': 0.6429282720750676}. Best is trial 13 with value: 11.609206918890969.\n",
      "LightGBM best params: {'learning_rate': 0.06262146074968952, 'num_leaves': 20, 'feature_fraction': 0.71949020633809, 'bagging_fraction': 0.8082009826745642}\n",
      "RF tuned results: {'MAE': 1.3742214063097287, 'RMSE': 3.2785987681231403, 'R2': 0.950371933817461}\n",
      "LightGBM tuned results: {'MAE': 1.6446795123758249, 'RMSE': 4.576044769111908, 'R2': 0.9307325266928737}\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Neural Network results: {'MAE': 44.78876891052246, 'RMSE': 2785.0326467891227, 'R2': -41.15696835465627}\n",
      "Saved final evaluation results to outputs/final_evaluation_results.csv\n",
      "Best model by RMSE: linear_regression\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab9aa9da51f4eb79bc1bbeb631536e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure: outputs\\figs\\shap_summary_linear_regression.png\n",
      "Saved SHAP feature importance for linear_regression\n",
      "Pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "run_full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297532bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
